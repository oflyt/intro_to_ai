{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(observation, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(observation, cmap='gray')\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "def downsize(img_arry):\n",
    "    img_arry = resize(img_arry, (84, 84), anti_aliasing=True)\n",
    "    return img_arry # [:,:84]\n",
    "\n",
    "def rgb2gray(img_arr):\n",
    "    return np.dot(img_arr[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def preprocess_image(img_arr):\n",
    "    downsized = downsize(observation)\n",
    "    return rgb2gray(downsized)\n",
    "\n",
    "def transform_reward(reward):\n",
    "        return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    def __init__(self, size):\n",
    "        # Pro-tip: when implementing a ring buffer, always allocate one extra element,\n",
    "        # this way, self.start == self.end always means the buffer is EMPTY, whereas\n",
    "        # if you allocate exactly the right number of elements, it could also mean\n",
    "        # the buffer is full. This greatly simplifies the rest of the code.\n",
    "        self.data = [None] * (size + 1)\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start and yet we just added one element. This means the buffer has one\n",
    "        # too many element. Remove the first element by incrementing start.\n",
    "        if self.end == self.start:\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[(self.start + idx) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.end < self.start:\n",
    "            return self.end + len(self.data) - self.start\n",
    "        else:\n",
    "            return self.end - self.start\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f35c797f20d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0matari_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "\n",
    "def atari_model(action_size, state_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16,\n",
    "                            8,\n",
    "                            strides=(4, 4),\n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\",\n",
    "                            input_shape=state_size,\n",
    "                            data_format=\"channels_first\"))\n",
    "    model.add(Conv2D(32,\n",
    "                            4,\n",
    "                            strides=(2, 2),\n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\",\n",
    "                            input_shape=state_size,\n",
    "                            data_format=\"channels_first\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(action_size))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
    "    \"\"\"\n",
    "    Do one deep Q learning iteration.\n",
    "\n",
    "    Params:\n",
    "    - model: The DQN\n",
    "    - gamma: Discount factor (should be 0.99)\n",
    "    - start_states: numpy array of starting states\n",
    "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
    "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
    "\n",
    "    \"\"\"\n",
    "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
    "    # The Q values of the terminal states is 0 by definition, so override them\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
    "    # the targets by the actions.\n",
    "    model.fit(\n",
    "        [start_states, actions], actions * Q_values[:, None],\n",
    "        nb_epoch=1, batch_size=len(start_states), verbose=0\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration(env, model, state, iteration, memory):\n",
    "    # Choose epsilon based on the iteration\n",
    "    epsilon = iteration # get_epsilon_for_iteration(iteration)\n",
    "\n",
    "    # Choose the action \n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = choose_best_action(model, state)\n",
    "\n",
    "    # Play one game iteration (note: according to the next paper, you should actually play 4 times here)\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    memory.add(state, action, new_frame, reward, is_done)\n",
    "\n",
    "    # Sample and fit\n",
    "    batch = memory.sample_batch(32)\n",
    "    model = fit_batch(model, batch)\n",
    "    return model\n",
    "    \n",
    "def choose_best_action(model, state):\n",
    "    print(state)\n",
    "    actions = model.predict(state)\n",
    "    action = np.argmax(actions)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_15_input to have 4 dimensions, but got array with shape (84, 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c8ffa2b2b106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mshow_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RL-Atari\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mq_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mring_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# your agent here (this takes random actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4b07d6b9ca09>\u001b[0m in \u001b[0;36mq_iteration\u001b[1;34m(env, model, state, iteration, memory)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_best_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Play one game iteration (note: according to the next paper, you should actually play 4 times here)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4b07d6b9ca09>\u001b[0m in \u001b[0;36mchoose_best_action\u001b[1;34m(model, state)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchoose_best_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_15_input to have 4 dimensions, but got array with shape (84, 84)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "# itertion = 0\n",
    "model = atari_model(env.action_space.n, (4,84,84))\n",
    "ring_buffer = RingBuf(size=10000)\n",
    "\n",
    "for t in range(500):\n",
    "    observation = env.render(mode='rgb_array')\n",
    "    state = preprocess_image(observation)\n",
    "    show_state(state, 0, \"RL-Atari\")\n",
    "    \n",
    "    q_iteration(env, model, state, epsilon, ring_buffer)\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100001):\n",
    "#     state = env.reset()\n",
    "\n",
    "#     epochs, penalties, reward, = 0, 0, 0\n",
    "#     done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        q_iteration(env, model, state, epsilon, memory)\n",
    "#         if random.uniform(0, 1) < epsilon:\n",
    "#             action = env.action_space.sample() # Explore action space\n",
    "#         else:\n",
    "#             action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
